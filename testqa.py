# -*- coding: utf-8 -*-
"""testQA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/135XUq92smFyxr96tqsDrLDzLF9zimXWD
"""
import os
import pandas as pd
import matplotlib.pyplot as plt
from transformers import GPT2TokenizerFast
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.chains import ConversationalRetrievalChain

openai_key='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'

'''loader=PyPDFLoader("Environmental Pollution.pdf")
pages=loader.load_and_split()
print(pages[1])

chunks=pages'''



from langchain.text_splitter import RecursiveCharacterTextSplitter
import textract
doc=textract.process('Environmental Pollution.pdf')

with open('Environment Pollution.txt','w') as f:
  f.write(doc.decode('utf-8'))

with open('Environment Pollution.txt','r') as f:
  text=f.read()

tokenizer=GPT2TokenizerFast.from_pretrained("gpt2")

def count_token(text:str)->int:
  return len(tokenizer.encode(text))

text_splitter=RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=24,
    length_function=count_token,
)

chunks=text_splitter.create_documents([text])

embeddings=OpenAIEmbeddings(openai_api_key=openai_key)

db=FAISS.from_documents(chunks,embeddings)

query="what are types of pollutions?"

docs=db.similarity_search(query)
docs[0]

